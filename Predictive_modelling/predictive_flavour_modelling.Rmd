---
title: "Predictive Sensory Modelling"
author: "Joshua Lomax"
date: "`r Sys.Date()`"
output: html_document
---
```{css, echo=FALSE}
body{
  font-family: Helvetica;
  font-size: 12pt;
}

```

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```
### Packages

``` {r load packages, echo = T}
required_packages <- c("tidyverse", "readxl", "reshape2", "openxlsx", "readr", "AMR", # data processing
                       "ggplot2", "ggrepel", "corrplot", "gridExtra", "GGally", # Plotting
                       "factoextra", "FactoMineR", "survminer", "agricolae", # Multivariate Analysis
                        "caret", "randomForest", "MultivariateRandomForest", #linear models
                       "gbm", "BGLR"
                       )
pacman::p_load(char = basename(required_packages), install = F)

devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R") # Ido's gist
```
<br>

### data preprocessing

<br>
The first step is to create a data matrix with all of our sensory and metabolite data. The matrix can then be normalised and na's can be imputed.

``` {r preprocessing, echo = T, eval = F}
# Extract SD1_Original_Data sheet from SD1_dataset_tomato.xlsx file
tom_og <- read_excel("data/supplemental_datasets/SD1_dataset_tomato.xlsx", sheet = "SD1_Original_Data")

# Remove columns not needed for most computation
tom_og <- tom_og %>% select(-species, -`panel number`)

# For the following metabolites, not every sample in the population has been quantified (e.g. missing data)
mets_with_missing_data <- c("firmness",
                            "citric",
                            "fructose",
                            "glucose",
                            "sucrose",
                            "Limonene",
                            "Nerylacetone")

# Thus, we will impute any missing data with the mean of the non-missing samples in the population

# Calculate means of non-missing samples for each metabolite containing missing data
means <- tom_og %>% 
  select(mets_with_missing_data) %>% 
  colMeans(na.rm = T)

# For each metabolite with missing data we find which samples are NAs 
#   and replace the NAs with the mean of the non-NA samples

for(i in 1:length(mets_with_missing_data)){
  tom_og[is.na(bb_og[, mets_with_missing_data[i]]), mets_with_missing_data[i]] <- means[i]
}

# Scale all features and responses to N(0,1)
tom_og[,2:length(tom_og)]  <- scale(tom_og[,2:length(tom_og)])

# Write new scaled/imputed data to .csv file for clean input to other scripts
write.csv(tom_og, "./data/input/tom_imputed_scaled.csv", row.names = F)


```

<br>

### Sensory characterisation data visualisation

<br>
In the case that we have trained panel data, a great way to visualise the data is to use a PCA. 
``` {r sensory pca, echo = T}
genotype_table <- read_excel("data/papaya_sample_meta_data.xlsx", 
                             sheet = "meta_data") 

complete_matrix <- read_excel("data/complete_data_matrix.xlsx", 
                              sheet = "Sheet1")

initial_pca_data <- complete_matrix %>% 
                    column_to_rownames("ID") 

# choose the number of components using your favourite method (e.g. elbow or straight line visual method)
initial_pca <- PCA(initial_pca_data, 
                   scale.unit = TRUE, 
                   graph = T, 
                   quali.sup = 1:3)

# or Visualize the eigenvalues/variances of the dimensions
fviz_screeplot(initial_pca, ncp=15) 

# choose the number of components using your favourite method (e.g. elbow or straight line visual method)

```
<br>
Using the straight line visual method we will choose ncp = 7
<br>
``` {r PCAplots, echo = T}
# adjust preliminary PCA with updated data

sensory_meta <- data.frame("var" = c("aroma_intensity_AR","sweet_fruit_AR","musty_off_note_AR",
                                     "fishy_AR","citrus_AR","floral_AR", "resistance_TX",
                                     "velvety_TX","juiciness_TX","dissolving_TX","fibrous_TX",
                                     "flavour_intensity_FL","sweetness_FL","bitterness_FL",
                                     "musty_FL","floral_FL", "bitter_AT","sweet_AT",
                                     "metallic_AT","prickly_AT"),
                           
                           "group" = c("aroma","aroma","aroma","aroma","aroma","aroma",
                                       "texture","texture","texture","texture","texture",
                                       "flavour","flavour","flavour","flavour","flavour",
                                       "aftertaste","aftertaste","aftertaste","aftertaste"))

# You may need to remove some variables due to preliminary data analysis 

# e.g., panelists failed to score samples consistently using a descriptor or there were technical issues measuring metabolites

No_aroma <- c("P-cymene", "Beta-Cyclocitral","Terpinene","Methyl_octanoate")

cutvariables <- c("prickly_AT", "fibrous_TX", "metallic_AT", No_aroma) 

PCA_data <- complete_matrix %>% 
            column_to_rownames("ID") %>% 
            select(-(all_of(cutvariables)))

# store a PCA using the appropriate variables and number of dimensions
PCA_vocs <- PCA(PCA_data, 
                scale.unit = TRUE, 
                ncp = 7, 
                graph = FALSE, 
                quali.sup = 1:27, # omit columns from PCA; e.g. metadata
                ind.sup = 47:69) #  omit rows from PCA; remove unnecessary samples

# A biplot can be used to visualise this data

# The factoextra package has a default command to create a biplot
# but it can be difficult to customise for aesthetics
fviz_pca_biplot(PCA_vocs)
```

``` {r ggplot PCA}
# make a data frame of top variables based on visual assessment

topvars <- PCA_vocs$var$contrib [,1:2] %>% # use the first two dimensions of the PCA
           as.data.frame() %>% 
           rownames_to_column(var = "species") %>% #move variable names to a column: "species"
           mutate (contribution = Dim.1 + Dim.2) %>% # create a column with combined contribution metric                                                           for each variable
           top_n(20, contribution) # filter list of variables to include the 20 most discerning variables                                       in the PCA

# make a dataframe with variable coordinates for arrows
PCAvar_coords <- PCA_vocs$var$coord[topvars$species,1:2] %>% #coordinates for top variables
                 as.data.frame() %>%                         
                 rownames_to_column(var = "var") %>% 
                 left_join(sensory_meta) %>% 
                 mutate(Category = snakecase::to_title_case(group),
                 sensory_label = snakecase::to_sentence_case(sub("_[A-Z]+$", "", var))) 

# create a scale value to the size of the scatter plot to adjust variable coordinates
var <- facto_summarize(PCA_vocs, 
                       element = "var", 
                       result = c("coord", "contrib", "cos2"))

colnames(var)[2:3] <- c("x", "y")

ind <- data.frame(PCA_vocs$ind$coord[, drop = FALSE], stringsAsFactors = TRUE)
colnames(ind) <- c("x", "y")

r <- min((max(ind[, "x"]) - min(ind[, "x"])/(max(var[, "x"]) - min(var[, "x"]))), 
         (max(ind[, "y"]) - min(ind[, "y"])/(max(var[,"y"]) - min(var[, "y"]))))

# update the variable coordinates
PCAvar_plot_data <- PCAvar_coords %>% 
                    as_tibble() %>% 
                    mutate(across(starts_with("Dim"), ~r*0.7*.x))

# get coordinates for individuals to map group means with meta data for aesthetics
PCAvoctable <- PCA_vocs$ind$coord %>% 
               as.data.frame() %>% 
               rownames_to_column("ID") %>% 
               left_join(genotype_table)

#create a table with the average coordinates for each phenotype
Mean_coords <- PCAvoctable %>% 
               group_by(Genotype,Flesh) %>% 
               summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))

# choose colours
my_colours <- c("#E31A1C","gold2")

# Get component variance information for axis labels
percentVar <- PCA_vocs$eig[1:2,2]

#plot PCA coords using ggplot

ggplot(Mean_coords) + # create a plot using our data points
  aes(x = Dim.1, y = Dim.2, fill = Flesh) + # assign data to axis and meta data to fill
    
  geom_hline(yintercept = 0, # add horizontal line along x-axis; dimension 1
             alpha = 0.5, 
             linetype = "dashed", 
             linewidth = 0.5) +
  geom_vline(xintercept = 0, # add vertical line along y-axis; dimension 2
             alpha = 0.5, 
             linetype = "dashed", 
             linewidth = 0.5) +

  geom_segment(mapping = aes(colour = Category, #add arrows for variables
                             xend = Dim.1, 
                             yend = Dim.2), 
               data = PCAvar_plot_data,
               inherit.aes = F,
               x = 0, y = 0, linewidth = 0.4,
               arrow = arrow(angle = 30, length = unit(0.2, "cm"))) +
  
  geom_text_repel(mapping = aes(colour = Category, #add text to variable arrows
                                x = Dim.1, 
                                y = Dim.2, 
                                label = sensory_label),
                  data = PCAvar_plot_data,
                  inherit.aes = F,
                  hjust = -0.09, vjust = -0.6, size = 3) +
  
  scale_color_manual(values = c("#8B0000", "#000080", "#228B22", "#708090")) + #colours for variable arrows
  
  geom_point(shape = 21, alpha = 1, size=2.5) + # modify data points
  scale_fill_manual(values = my_colours) +
  
  geom_text_repel(label = Mean_coords$Genotype, # add text to data points
                  size=3, hjust=0.7, vjust=-0.2,
                  colour = "black", fontface = "italic") +
  xlim(-5.3, 6.7) + ylim(-3.8,5.2) + # adjust x- and y-axis to fit arrows and text
  
  theme_bw() + # remove background, add border, remove grid lines and change title size
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title = element_text(size=12)) +
  
  guides(colour=guide_legend(override.aes = list(label="")), # modify legend symbols
         text="none") +
  
  labs( x=glue::glue("C1: {round(percentVar[1],2)}% variance"), # modify axis and legend titles
        y=glue::glue("C2: {round(percentVar[2],2)}% variance"), fill="Flesh", 
        colour="Attribute") 

```
<br>
This code can be used as a template to also plot consumer data. Such as the papaya consumption habits of the participants compared with each groups over-all liking of the papaya samples.
<br>
<br>
<br>
<br>
![](figures/consumption_liking.png)
<br>

### Consumer survey data visualisation 

<br>
In this case the aim is to identify consumer liking as a function of papaya variety. 120 panelists have tasted 9 papaya varieties and rated them on a scale from 0 to 100 for how much they like the fruit; we also have information regarding the participants usual papaya consumption habit. After completing an ANOVA it is apparent that both **variety** and **habit** significantly affect the consumers liking score. Typically a boxplot is appropriate to compare means but with sensory surveys they standard deviation can often cover most of the 0 to 100 scale. In this case, a violin plot can be overlaid to give an indication of the distribution of consumer responses.
<br>
```{r sensory liking plots, echo=TRUE}
liking_data <- read_xlsx("data/consumer_data/consumer_data.xlsx", 
                         sheet = "all_data") %>% 
                mutate(age = as.numeric(age)) %>% 
                mutate(age_brackets = age_groups(age, split_at = "tens"),
                       consumption = as.character(consumption))

key <- read_xlsx("data/consumer_data/consumer_data.xlsx", sheet = "key")

factorial_model <- aov(liking ~ sample*age*consumption, data = liking_data)
summary(factorial_model)

tHSD <- HSD.test(factorial_model, "sample")
con_tHSD <- HSD.test(factorial_model, "consumption") 
nine_colours <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
                  "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22")
#   boxplot

ranges <- data.frame(tHSD$means) %>% rownames_to_column(var = "sample")
groups <- data.frame(tHSD$groups) %>% rownames_to_column(var = "sample") %>% select(-liking)
label_df <- left_join(ranges, groups, join_by(sample)) %>% mutate(y = Max + 5)

#Generate ggplot

ggplot(liking_data, aes(x = reorder(sample,+liking), y = liking)) + 
  geom_boxplot(aes(fill = sample)) +
  geom_text(data = label_df, aes(x = sample, y = y, 
                                 label = groups, color = sample)) +
  scale_color_manual(values = nine_colours) +
  scale_fill_manual(values = nine_colours) +
  scale_y_continuous(expand = c(0,0), limits = c(0,110), 
                     breaks = c(20,40,60,80,100)) +
  coord_flip() + 
  theme_bw()+
  theme(legend.position = "none", panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), panel.background = element_blank(), 
        axis.line = element_line(colour = "black")) +
  labs(y="Consumer liking score", x="Genotype")

# violin and boxplot plot

ggplot(liking_data, aes(x = reorder(sample,+liking), y = liking)) + 
  geom_violin(width=0.95,aes(fill = sample)) +
  geom_boxplot(width=0.1, color="#404040", alpha=0.2) +
  geom_text(data = label_df, aes(x = sample, y = y, 
                                 label = groups, color = sample), size = 5) +
  scale_color_manual(values = nine_colours) +
  scale_fill_manual(values = nine_colours) +
  scale_y_continuous(expand = c(0,0), limits = c(0,110), 
                     breaks = c(20,40,60,80,100)) +
  coord_flip() + theme_bw()+
  theme(legend.position = "none", panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), panel.background = element_blank(), 
        axis.line = element_line(colour = "black"), axis.text=element_text(size=12),
        axis.title=element_text(size=12)) +
  labs(y="Consumer liking score", x="Genotype")
#ggsave("figure/consumer_liking/liking_violin_plot2.pdf", width = 10, height = 8)

```
<br>
<br>
### Metabolite data visualisation
<br>
This example from [Colantonio et al. (2021)](https://www.pnas.org/doi/full/10.1073/pnas.2115865119) measures the volatiles from 206 tomato accession in a breeding population. This visualisation gives us a distribution of each metabolite class across the population.
<br>
```{r hisorgram prep, echo=F, results='hide'}
# Load data
#key = read_excel("data/input/Metabolites_cluster.xlsx", sheet = 1)
key = read.csv("data/input/tom_metabolites_clusters_key.csv")
tom = tibble(read.csv("data/input/tom_imputed.csv", check.names = F))


# Make unique genotype ids
tom$id <- paste(1:nrow(tom), tom$id)

# Gathering names of traits and metabolites
sensory = c("liking","sweetness", "sour", "umami", "intensity")
mets = colnames(tom)[!colnames(tom) %in% c(sensory, "id")]

# Check for the ones that don't overlap
key$Metabolite[!key$Metabolite %in% mets]
mets[!mets %in% key$Metabolite]

# Creating a metabolite key by merging
idx.tom = data.frame(Metabolite = mets) %>%
  merge(., key, by="Metabolite")

# Selecting only metabolite concentrations
tmp <- tom %>% select(-all_of(sensory))

# Creating the optimal data frame for plotting
tmp2 <- data.frame(t(tmp[, -1]))
colnames(tmp2) <- tmp$id
tmp2$id <- rownames(data.frame(tmp2))

tmp3 <- merge(tmp2, idx.tom[,c("Metabolite", "fig1b_histogram")], by.x = "id", by.y = "Metabolite", all.x = T)

tmp3$fig1b_histogram <- as.factor(tmp3$fig1b_histogram)
colnames(tmp3)[1] <- "metabolite"

tmp4 <- tmp3 %>% #select(-fig1b_histogram) %>% 
  pivot_longer(cols = c(-metabolite, -fig1b_histogram), 
               names_to = "genotype", 
               values_to = "concentration") %>% 
  drop_na() %>% 
  filter(!(fig1b_histogram %in% c("Acid/Sugar", "unknown"))) %>% 
  filter(concentration != 0)
```

```{r histogram, echo = T, results='hide', fig.cap = "*Distribution of metabolite concentrations for each volatile group across the tomato population. Volatile concentrations are reported in nanograms per gram fresh weight per hour (ng/gfw/h) on a log10scale.*"}


# Plotting
tmp4 %>% 
  ggplot(., aes(x = fig1b_histogram, 
                y = log((concentration), 10), 
                fill = fig1b_histogram)) + 
  geom_violin(color = "black", size = 0.5) +
  ylab(expression(paste(log[10], bgroup("[", ng/gfw/hr, "]")))) +
  scale_y_continuous(limits = c(-3.25, 3.25), 
                     breaks = seq(-3, 3, by = 1)) +
  scale_fill_manual(values = c("#9999ffff", 
                               "#74a9ccff", 
                               "#ffcc99ff", 
                               "#e19582ff", 
                               "#ccccccff")) +
  labs(fill='Metabolite Class') +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 14),
        axis.ticks.x = element_blank()) 

```
<br>
Although correlation doesn't imply causation it can be an important place to start before predictive modeling. Exploring correlations within the dataset can be reported in a table or using corrplot. 
<br>
``` {r corrplot}
cor_matrix <- read_excel("data/complete_data_matrix.xlsx") %>% 
              filter(!assay == "Feb23") %>% 
              column_to_rownames("ID") %>% 
              select(Hexanal:prickly_AT) %>% 
              as.matrix()
# this data contains non-normally distributed data so the non-parametric Kendall method in cor
res <- cor(cor_matrix, method = "kendall")
res1 <- cbind(" "=rownames(round(res, 2)), round(res, 2)) #round values in correlation matrix

#write_xlsx(res1, "results/correlation.xlsx", sheet = "Aug22", overwritesheet = T)

# Visual representation of correlation matrix
corrplot(res, type = "upper", order = "FPC", 
         tl.col = "black", tl.srt = 45,tl.cex = 0.5)

```
<br>
<br>

## Predictive modelling

**Some things to consider**

  - There's no such thing as a perfect model
  - Using many features or variables can lead to over-fitting
  - Feature engineering can improve the model efficiency and accuracy
  - [Feature engineering Blog](https://domino.ai/data-science-dictionary/feature-engineering)
  - [Review dealing with complex data and modelling methods](https://link.springer.com/chapter/10.1007/978-981-19-2416-3_1)

![*From links above*](results/feature_engineering.png)
<br>
<br>
<br>
<br>
<br>
Feature engineering involves transforming raw data into a format that enhances the performance of machine learning models. The key steps in feature engineering include:

  - **Data Exploration and Understanding**: Explore and understand the dataset, including the types
    of features and their distributions. Understanding the shape of the data is key.
    
  - **Handling Missing Data**: Address missing values through imputation or removal of instances or     features with missing data. There are many algorithmic approaches to handling missing data.
  
  - **Variable Encoding**: Convert categorical variables into a numerical format suitable for 
    machine learning algorithms using methods.
    
  - **Feature Scaling**: Standardize or normalize numerical features to ensure they are on a 
    similar scale, improving model performance.
    
  - **Feature Creation**: Generate new features by combining existing ones to capture relationships between variables.
  
  - **Handling Outliers**: Identify and address outliers in the data through techniques like trimming or transforming the data.
  
  - **Normalization**: Normalize features to bring them to a common scale, important for algorithms sensitive to feature magnitudes.
  
  - **Binning or Discretization**: Convert continuous features into discrete bins to capture specific patterns in certain ranges.
  
  - **Text Data Processing**: If dealing with text data, perform tasks such as tokenization, stemming, and removing stop words.
  
  - **Time Series Features**: Extract relevant timebased features such as lag features or rolling statistics for time series data.
  
  - **Vector Features**: Vector features are commonly used for training in machine learning. In machine learning, data is represented in the form of features, and these features are often organized into vectors. A vector is a mathematical object that has both magnitude and direction and can be represented as an array of numbers.
  
  - **Feature Selection**: Identify and select the most relevant features to improve model interpretability and efficiency using techniques like univariate feature selection or recursive feature elimination.
  
  - **Feature Extraction**: Feature extraction aims to reduce data complexity (often known as “data dimensionality”) while retaining as much relevant information as possible. This helps to improve the performance and efficiency of machine learning algorithms and simplify the analysis process. Feature extraction may involve the creation of new features (“feature engineering”) and data manipulation to separate and simplify the use of meaningful features from irrelevant ones. Create new features or reduce dimensionality using techniques such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-DSNE).
  
  - **Cross-validation**: selecting features prior to cross-validation can introduce significant bias. Evaluate the impact of feature engineering on model performance using cross-validation techniques.
  
<br>
<br>
<br>
<br>
<br>

### Linear regression example - Random Forest
<br>
```{r, modelplot setup}
genotype_table <- read_excel("data/papaya_sample_meta_data.xlsx", sheet = "VOCs")

model_matrix <- read_excel("data/complete_data_matrix.xlsx", sheet = "Sheet1") %>% 
                 select(-(1:3)) %>% 
                 column_to_rownames("ID") %>% 
                 as.data.frame()

scaled_matrix <- model_matrix %>% 
                 scale() %>% 
                 as.data.frame()

metabolite_data <- scaled_matrix %>% 
                select((1:28))

sensory_data <- scaled_matrix %>% 
                   select((29:48))

```


First, we can use the package caret to train a random forest model

```{r random forest train}
data <- model_matrix %>% select(-(29:33),-(35:48))

indexes <-  createDataPartition(data$floral_AR, p = .90, list = F)
train <-  data[indexes, ]
test <-  data[-indexes, ]
# Define the control
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")
# Run the model
rf_default <- train(floral_AR~.,
                    data = train,
                    method = "rf",
                    metric = "RMSE",
                    trControl = trControl)
# Search best mtry: You can test the model with values of mtry from 1 to 10
set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(1: 15))
rf_mtry <- train(floral_AR~.,
                 data = train,
                 method = "rf",
                 metric = "RMSE",
                 tuneGrid = tuneGrid,
                 trControl = trControl,
                 importance = TRUE,
                 nodesize = 14,
                 ntree = 300)
print(rf_mtry)
# The best value of mtry is stored in:
rf_mtry$bestTune$mtry

# You can store it and use it when you need to tune the other parameters.
max(rf_mtry$results$Rsquared)
best_mtry <- rf_mtry$bestTune$mtry 
best_mtry

# You need to create a loop to evaluate the different values of maxnodes. In the following code, you will:
# Create a list
# Create a variable with the best value of the parameter mtry; Compulsory
# Create the loop
# Store the current value of maxnode
# Summarize the results
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
for (maxnodes in c(10: 30)) {
  set.seed(1234)
  rf_maxnode <- train(floral_AR~.,
                      data = train,
                      method = "rf",
                      metric = "RMSE",
                      tuneGrid = tuneGrid,
                      trControl = trControl,
                      importance = TRUE,
                      nodesize = 14,
                      maxnodes = maxnodes,
                      ntree = 300)
  current_iteration <- toString(maxnodes)
  store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry)

# Now that you have the best value of mtry and maxnode, you can tune the number of trees. The method is exactly the same as maxnode.

store_maxtrees <- list()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
  set.seed(5678)
  rf_maxtrees <- train(floral_AR~.,
                       data = train,
                       method = "rf",
                       metric = "RMSE",
                       tuneGrid = tuneGrid,
                       trControl = trControl,
                       importance = TRUE,
                       nodesize = 14,
                       maxnodes = 30,
                       ntree = ntree)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)

# You have your final model. You can train the random forest with the following parameters:
# ntree =1000: 1000 trees will be trained
# mtry=15: 15 features is chosen for each iteration
# maxnodes = 30: Maximum 24 nodes in the terminal nodes (leaves)
fit_rf <- train(floral_AR~.,
                train,
                method = "rf",
                metric = "RMSE",
                tuneGrid = tuneGrid,
                trControl = trControl,
                importance = TRUE,
                nodesize = 14,
                ntree = 1000,
                maxnodes = 30)
# The library caret has a function to make prediction.
prediction <-predict(fit_rf, test)
forest_values <- test %>% cbind(prediction)

# Accuracy can be estimated by comparing predicted floral aroma with actual floral aroma
predicted <- forest_values$floral_AR
actual <- forest_values$prediction

accuracy <- 100-(((predicted-actual)/(actual))*100)
accuracy_converted <- abs(accuracy) %>% -100 %>% abs() %>% as_tibble()
ave_accuracy <- accuracy_converted %>% summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))
model_accuracy <- 100-ave_accuracy
print(model_accuracy)
```


## Model Statistics
<br>

### Gradient Boosted Model (GBM)

<br>

Gradient boosted model generates relative variable importance scores for each characteristic. It is possible to filter aroma causing compounds by aroma activity values that are based on estimates for odor threshold concentrations to reduce the number of variables. That is, only including aroma compounds with concentrations that exceed the theoretical odor threshold in at least one sample group. For gradient-boosting machines the variable importance repre-sents the marginal effect of that chemical including the interac-tion effects with other chemicals. This value is scaled between 0and 100 where 0 is a not an important predictor and 100 is animportant predictor.

```{r GBM}
#gbm model

model_gbm <-  gbm(sensory_data$floral_AR ~.,
                  data = metabolite_data,
                  distribution = "gaussian",
                  cv.folds = 10,
                  shrinkage = c(0.05),
                  n.minobsinnode = c(10),
                  n.trees = c(500), 
                  interaction.depth = c(5), 
                  set.seed(123))

model_summary <- summary.gbm(model_gbm) %>% 
                 mutate(var = (gsub("`","",var))) %>% 
                 rename("species" = "var", "floral_aroma_gbm" = "rel.inf")
```


### Bayesian Model
The correlation of compounds to aroma intensity can be predicted by the beta coefficient from Bayesian modelling.In BayesA, the beta coefficients indicate the individual additive effect of that chemical free of interactions. This coefficient predicts if achemical is important for enhancing the flavor attribute (positive value) or decreasing the flavor attribute (negative value).

```{r BayesA}
# BayesA model

y <- sensory_data$floral_AR

ETA <- list(list(X = metabolite_data, model = 'BayesA'))

fit_BA <- BGLR(y = y, ETA = ETA, nIter = 3000, burnIn = 1000,
               thin = 3, saveAt = '', df0 = 5, S0 = NULL,
               weights = NULL, R2 = 0.5)
# add Bayes A beta coefficients to model summary
model_plot_data <- model_summary %>% 
                  mutate(b_floral_aroma = fit_BA$ETA[[1]]$b) %>% 
                  left_join(genotype_table)
```

Beta coefficients from the model are added to the table containing relative variable importance from the previous model. Finally, GBM values (x-axis) and beta-coefficients (y-axis) can be plotted to visualise the determinants of each characteristic.
<br>

### Model Plot

<br>

```{r modelplot}

# plotting

label_subset <- model_plot_data %>% 
                subset(b_floral_aroma > 0.2 | b_floral_aroma < -0.18) %>% 
                bind_rows(subset(model_plot_data, floral_aroma_gbm > 10)) %>% 
                distinct()
#choose colours

my_colours <- c("#6a3d9a", "#ff7f00", "#e31a1c", "#33a02c", "#1f78b4")

#plot PCA coords using ggplot

ggplot(model_plot_data) + 
  aes(x = floral_aroma_gbm, y = b_floral_aroma, fill = Class2, label = species) + 
  geom_hline(yintercept = 0, alpha = 0.6, 
             linetype="dashed", linewidth=0.5) +
  scale_fill_manual(values = my_colours)+
  xlim(0, 20) + ylim(-0.3,0.5) + 
  geom_point(shape = 21,alpha = 0.7, size=2.5) +
  theme_bw() + theme(plot.background = element_blank(), 
                     panel.grid.major = element_blank(), 
                     panel.grid.minor = element_blank(),
                     axis.title = element_text(size=12),
                     plot.title = element_text(hjust = 0.5)) +
  geom_text_repel(data = label_subset, 
                  x=label_subset$floral_aroma_gbm, y=label_subset$b_floral_aroma, 
                  inherit.aes = F, 
                  label = label_subset$species,
                  hjust=-0.15, vjust=0.5, size=3.2)+
  labs( x="Variable importance (GBM)",
        y="β coefficient (BayesA)", fill="Metabolite type",
        title = "Floral Aroma") 

```